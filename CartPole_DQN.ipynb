{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動画の描画\n",
    "\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0),\n",
    "               dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
    "                                   interval=50)\n",
    "\n",
    "    anim.save('movie_cartpole_DQN.mp4')  # 動画のファイル名と保存\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定数の設定\n",
    "ENV = 'CartPole-v0'  # 使用する課題名\n",
    "GAMMA = 0.99  # 時間割引率\n",
    "MAX_STEPS = 200  # 1試行のstep数\n",
    "NUM_EPISODES = 5000  # 最大試行回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 経験を保存するメモリクラスを定義\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY  # メモリの最大長さ\n",
    "        self.memory = []  # 経験を保存する変数\n",
    "        self.index = 0  # 保存するindexを示す変数\n",
    "\n",
    "    def push(self, state, action, state_next, reward):\n",
    "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
    "\n",
    "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントが持つ脳となるクラスです、DQNを実行します\n",
    "# Q関数をディープラーニングのネットワークをクラスとして定義\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "CAPACITY = 10000\n",
    "\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('fc1', nn.Linear(num_states, 64))\n",
    "        self.model.add_module('relu1', nn.ReLU())\n",
    "        self.model.add_module('fc2', nn.Linear(64, 64))\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(64, num_actions))\n",
    "\n",
    "        print(self.model)  # ネットワークの形を出力\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "\n",
    "    def replay(self):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 1. メモリサイズの確認\n",
    "        # -----------------------------------------\n",
    "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2. ミニバッチの作成\n",
    "        # -----------------------------------------\n",
    "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # 2.2 各変数をミニバッチに対応する形に変形\n",
    "        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
    "        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
    "        # これをミニバッチにしたい。つまり\n",
    "        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
    "        # それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "        # catはConcatenates（結合）のことです。\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        # -----------------------------------------\n",
    "        # 3.1 ネットワークを推論モードに切り替える\n",
    "        self.model.eval()\n",
    "\n",
    "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
    "        # [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
    "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
    "        # それに対応するQ値をgatherでひっぱり出す。\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
    "\n",
    "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                                    batch.next_state)))\n",
    "        # まずは全部0にしておく\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        # 次の状態があるindexの最大Q値を求める\n",
    "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
    "        # そしてそのQ値（index=0）を出力します\n",
    "        # detachでその値を取り出します\n",
    "        next_state_values[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 4. 結合パラメータの更新\n",
    "        # -----------------------------------------\n",
    "        # 4.1 ネットワークを訓練モードに切り替える\n",
    "        self.model.train()\n",
    "\n",
    "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                                expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "        self.optimizer.step()  # 結合パラメータを更新\n",
    "\n",
    "    def decide_action(self, state, episode):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1, 1)\n",
    "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
    "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
    "\n",
    "        else:\n",
    "            # 0,1の行動をランダムに返す\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
    "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPoleのエージェントクラス＝棒付き台車そのもの\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        '''課題の状態と行動の数を設定する'''\n",
    "        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n",
    "\n",
    "    def update_q_function(self):\n",
    "        '''Q関数を更新する'''\n",
    "        self.brain.replay()\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        self.brain.memory.push(state, action, state_next, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPoleを実行する環境のクラス\n",
    "\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)  # 実行する課題を設定\n",
    "        num_states = self.env.observation_space.shape[0]  # 課題の状態数4を取得\n",
    "        num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n",
    "        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        episode_final = False  # 最後の試行フラグ\n",
    "        frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
    "        finished_step=[]\n",
    "        reward_means=[]\n",
    "        goal_average_reward=0.975\n",
    "        num_consecutive_iterations = 100 \n",
    "        total_reward_vec = np.zeros(num_consecutive_iterations)  #各試行の報酬を格納\n",
    "\n",
    "        for episode in range(NUM_EPISODES):  # 最大試行数分繰り返す\n",
    "            observation = self.env.reset()  # 環境の初期化\n",
    "            episode_reward = 0 \n",
    "\n",
    "            state = observation  # 観測をそのまま状態sとして使用\n",
    "            state = torch.from_numpy(state).type(torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
    "            state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "\n",
    "                if episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
    "                    frames.append(self.env.render(mode='rgb_array'))\n",
    "                   \n",
    "\n",
    "                action = self.agent.get_action(state, episode)  # 行動を求める\n",
    "\n",
    "                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "                # actionから.item()を指定して、中身を取り出す\n",
    "                observation_next, _, done, _ = self.env.step(action.item())  # rewardとinfoは使わないので_にする\n",
    "                \n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    state_next = None  # 次の状態はないので、Noneを格納\n",
    "\n",
    "                    if step < 199:\n",
    "                        reward = torch.FloatTensor([-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "                        episode_reward+=reward.numpy()\n",
    "                    else:\n",
    "                        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "                        episode_reward+=reward.numpy()\n",
    "                else:\n",
    "                    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                    episode_reward+=reward.numpy()\n",
    "                    state_next = observation_next  # 観測をそのまま状態とする\n",
    "                    state_next = torch.from_numpy(state_next).type(torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                    state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "                \n",
    "                # メモリに経験を追加\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                # Experience ReplayでQ関数を更新する\n",
    "                self.agent.update_q_function()\n",
    "\n",
    "                # 観測の更新\n",
    "                state = state_next\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward)) \n",
    "                    print('{0} Episode: Finished after {1} time steps  Episode Reward:{2}/Mean:{3}'.format(\n",
    "                        episode+1, step + 1,episode_reward,total_reward_vec.mean()))\n",
    "                    finished_step.append(step+1)\n",
    "                    reward_means.append(total_reward_vec.mean())\n",
    "                    #print(total_reward_vec)\n",
    "                    break\n",
    "\n",
    "            if episode_final is True:\n",
    "                # 動画を保存と描画\n",
    "                display_frames_as_gif(frames)\n",
    "                break\n",
    "\n",
    "          \n",
    "            if  (total_reward_vec.mean() >= goal_average_reward) or ((episode+1)==NUM_EPISODES-1):\n",
    "                episode_final = True  # 次の試行を描画を行う最終試行とする\n",
    "                print('学習完了')\n",
    "                \n",
    "                #グラフの表示（2スケールで出力）\n",
    "                x=np.linspace(1,episode+1,episode+1)\n",
    "                y1=finished_step\n",
    "                y2=reward_means\n",
    "                width1=((episode+1)/500)*6.4 #小スケールでの出力サイズ\n",
    "                width2=((episode+1)/150)*6.4 #大スケールでの出力サイズ\n",
    "                \n",
    "                fig1=plt.figure(figsize=(width1,4.8)) #達成ステップ数のグラフ画像\n",
    "                ax1=fig1.add_subplot(111)\n",
    "                ax1.plot(x,y1,lw=0.6)\n",
    "                ax1.set_xlabel('episode')\n",
    "                ax1.set_ylabel('max step')\n",
    "                ax1.set_xlim(1,episode+1)\n",
    "                ax1.xaxis.set_major_locator(mpl.ticker.MultipleLocator(100))\n",
    "                plt.grid(b=True, which='major',color='#999999', linestyle='-', alpha=0.4)\n",
    "\n",
    "                fig2=plt.figure(figsize=(width1,4.8)) #平均報酬のグラフ画像\n",
    "                ax2=fig2.add_subplot(111)\n",
    "                ax2.plot(x,y2,lw=0.6)\n",
    "                ax2.set_xlabel('episode')\n",
    "                ax2.set_ylabel('reward mean')\n",
    "                ax2.set_xlim(1,episode+1)\n",
    "                ax2.set_ylim(-1,1)\n",
    "                ax2.xaxis.set_major_locator(mpl.ticker.MultipleLocator(100))\n",
    "                plt.grid(b=True, which='major', color='#999999', linestyle='-', alpha=0.4)\n",
    "                \n",
    "                fig3=plt.figure(figsize=(width2,4.8)) #達成ステップ数のグラフ画像\n",
    "                ax3=fig3.add_subplot(111)\n",
    "                ax3.plot(x,y1,lw=0.6)\n",
    "                ax3.set_xlabel('episode')\n",
    "                ax3.set_ylabel('max step')\n",
    "                ax3.set_xlim(1,episode+1)\n",
    "                ax3.xaxis.set_major_locator(mpl.ticker.MultipleLocator(50))\n",
    "                plt.grid(b=True, which='major',color='#999999', linestyle='-', alpha=0.4)\n",
    "                \n",
    "                fig4=plt.figure(figsize=(width2,4.8)) #平均報酬のグラフ画像\n",
    "                ax4=fig4.add_subplot(111)\n",
    "                ax4.plot(x,y2,lw=0.6)\n",
    "                ax4.set_xlabel('episode')\n",
    "                ax4.set_ylabel('reward mean')\n",
    "                ax4.set_xlim(1,episode+1)\n",
    "                ax4.set_ylim(-1,1)\n",
    "                ax4.xaxis.set_major_locator(mpl.ticker.MultipleLocator(50))\n",
    "                plt.grid(b=True, which='major', color='#999999', linestyle='-', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "1 Episode: Finished after 14 time steps  Episode Reward:[-1.]/Mean:-0.01\n",
      "2 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-0.02\n",
      "3 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-0.03\n",
      "4 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.04\n",
      "5 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.05\n",
      "6 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.06\n",
      "7 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.07\n",
      "8 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.08\n",
      "9 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.09\n",
      "10 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.1\n",
      "11 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.11\n",
      "12 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.12\n",
      "13 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.13\n",
      "14 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.14\n",
      "15 Episode: Finished after 12 time steps  Episode Reward:[-1.]/Mean:-0.15\n",
      "16 Episode: Finished after 20 time steps  Episode Reward:[-1.]/Mean:-0.16\n",
      "17 Episode: Finished after 16 time steps  Episode Reward:[-1.]/Mean:-0.17\n",
      "18 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.18\n",
      "19 Episode: Finished after 12 time steps  Episode Reward:[-1.]/Mean:-0.19\n",
      "20 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-0.2\n",
      "21 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.21\n",
      "22 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.22\n",
      "23 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-0.23\n",
      "24 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.24\n",
      "25 Episode: Finished after 13 time steps  Episode Reward:[-1.]/Mean:-0.25\n",
      "26 Episode: Finished after 41 time steps  Episode Reward:[-1.]/Mean:-0.26\n",
      "27 Episode: Finished after 17 time steps  Episode Reward:[-1.]/Mean:-0.27\n",
      "28 Episode: Finished after 17 time steps  Episode Reward:[-1.]/Mean:-0.28\n",
      "29 Episode: Finished after 20 time steps  Episode Reward:[-1.]/Mean:-0.29\n",
      "30 Episode: Finished after 23 time steps  Episode Reward:[-1.]/Mean:-0.3\n",
      "31 Episode: Finished after 56 time steps  Episode Reward:[-1.]/Mean:-0.31\n",
      "32 Episode: Finished after 35 time steps  Episode Reward:[-1.]/Mean:-0.32\n",
      "33 Episode: Finished after 67 time steps  Episode Reward:[-1.]/Mean:-0.33\n",
      "34 Episode: Finished after 32 time steps  Episode Reward:[-1.]/Mean:-0.34\n",
      "35 Episode: Finished after 47 time steps  Episode Reward:[-1.]/Mean:-0.35\n",
      "36 Episode: Finished after 48 time steps  Episode Reward:[-1.]/Mean:-0.36\n",
      "37 Episode: Finished after 28 time steps  Episode Reward:[-1.]/Mean:-0.37\n",
      "38 Episode: Finished after 43 time steps  Episode Reward:[-1.]/Mean:-0.38\n",
      "39 Episode: Finished after 42 time steps  Episode Reward:[-1.]/Mean:-0.39\n",
      "40 Episode: Finished after 49 time steps  Episode Reward:[-1.]/Mean:-0.4\n",
      "41 Episode: Finished after 35 time steps  Episode Reward:[-1.]/Mean:-0.41\n",
      "42 Episode: Finished after 37 time steps  Episode Reward:[-1.]/Mean:-0.42\n",
      "43 Episode: Finished after 34 time steps  Episode Reward:[-1.]/Mean:-0.43\n",
      "44 Episode: Finished after 75 time steps  Episode Reward:[-1.]/Mean:-0.44\n",
      "45 Episode: Finished after 33 time steps  Episode Reward:[-1.]/Mean:-0.45\n",
      "46 Episode: Finished after 89 time steps  Episode Reward:[-1.]/Mean:-0.46\n",
      "47 Episode: Finished after 111 time steps  Episode Reward:[-1.]/Mean:-0.47\n",
      "48 Episode: Finished after 92 time steps  Episode Reward:[-1.]/Mean:-0.48\n",
      "49 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.47\n",
      "50 Episode: Finished after 170 time steps  Episode Reward:[-1.]/Mean:-0.48\n",
      "51 Episode: Finished after 119 time steps  Episode Reward:[-1.]/Mean:-0.49\n",
      "52 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.48\n",
      "53 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.47\n",
      "54 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.46\n",
      "55 Episode: Finished after 179 time steps  Episode Reward:[-1.]/Mean:-0.47\n",
      "56 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.46\n",
      "57 Episode: Finished after 173 time steps  Episode Reward:[-1.]/Mean:-0.47\n",
      "58 Episode: Finished after 158 time steps  Episode Reward:[-1.]/Mean:-0.48\n",
      "59 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.47\n",
      "60 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.46\n",
      "61 Episode: Finished after 151 time steps  Episode Reward:[-1.]/Mean:-0.47\n",
      "62 Episode: Finished after 169 time steps  Episode Reward:[-1.]/Mean:-0.48\n",
      "63 Episode: Finished after 199 time steps  Episode Reward:[-1.]/Mean:-0.49\n",
      "64 Episode: Finished after 161 time steps  Episode Reward:[-1.]/Mean:-0.5\n",
      "65 Episode: Finished after 154 time steps  Episode Reward:[-1.]/Mean:-0.51\n",
      "66 Episode: Finished after 145 time steps  Episode Reward:[-1.]/Mean:-0.52\n",
      "67 Episode: Finished after 139 time steps  Episode Reward:[-1.]/Mean:-0.53\n",
      "68 Episode: Finished after 159 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "69 Episode: Finished after 153 time steps  Episode Reward:[-1.]/Mean:-0.55\n",
      "70 Episode: Finished after 165 time steps  Episode Reward:[-1.]/Mean:-0.56\n",
      "71 Episode: Finished after 174 time steps  Episode Reward:[-1.]/Mean:-0.57\n",
      "72 Episode: Finished after 132 time steps  Episode Reward:[-1.]/Mean:-0.58\n",
      "73 Episode: Finished after 124 time steps  Episode Reward:[-1.]/Mean:-0.59\n",
      "74 Episode: Finished after 125 time steps  Episode Reward:[-1.]/Mean:-0.6\n",
      "75 Episode: Finished after 154 time steps  Episode Reward:[-1.]/Mean:-0.61\n",
      "76 Episode: Finished after 140 time steps  Episode Reward:[-1.]/Mean:-0.62\n",
      "77 Episode: Finished after 163 time steps  Episode Reward:[-1.]/Mean:-0.63\n",
      "78 Episode: Finished after 170 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "79 Episode: Finished after 192 time steps  Episode Reward:[-1.]/Mean:-0.65\n",
      "80 Episode: Finished after 196 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "81 Episode: Finished after 189 time steps  Episode Reward:[-1.]/Mean:-0.67\n",
      "82 Episode: Finished after 193 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "83 Episode: Finished after 173 time steps  Episode Reward:[-1.]/Mean:-0.69\n",
      "84 Episode: Finished after 183 time steps  Episode Reward:[-1.]/Mean:-0.7\n",
      "85 Episode: Finished after 178 time steps  Episode Reward:[-1.]/Mean:-0.71\n",
      "86 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.7\n",
      "87 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.69\n",
      "88 Episode: Finished after 189 time steps  Episode Reward:[-1.]/Mean:-0.7\n",
      "89 Episode: Finished after 183 time steps  Episode Reward:[-1.]/Mean:-0.71\n",
      "90 Episode: Finished after 190 time steps  Episode Reward:[-1.]/Mean:-0.72\n",
      "91 Episode: Finished after 167 time steps  Episode Reward:[-1.]/Mean:-0.73\n",
      "92 Episode: Finished after 194 time steps  Episode Reward:[-1.]/Mean:-0.74\n",
      "93 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.73\n",
      "94 Episode: Finished after 176 time steps  Episode Reward:[-1.]/Mean:-0.74\n",
      "95 Episode: Finished after 195 time steps  Episode Reward:[-1.]/Mean:-0.75\n",
      "96 Episode: Finished after 183 time steps  Episode Reward:[-1.]/Mean:-0.76\n",
      "97 Episode: Finished after 169 time steps  Episode Reward:[-1.]/Mean:-0.77\n",
      "98 Episode: Finished after 172 time steps  Episode Reward:[-1.]/Mean:-0.78\n",
      "99 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.77\n",
      "100 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.76\n",
      "101 Episode: Finished after 177 time steps  Episode Reward:[-1.]/Mean:-0.76\n",
      "102 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.74\n",
      "103 Episode: Finished after 188 time steps  Episode Reward:[-1.]/Mean:-0.74\n",
      "104 Episode: Finished after 171 time steps  Episode Reward:[-1.]/Mean:-0.74\n",
      "105 Episode: Finished after 189 time steps  Episode Reward:[-1.]/Mean:-0.74\n",
      "106 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.7\n",
      "108 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.68\n",
      "109 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.66\n",
      "110 Episode: Finished after 175 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "111 Episode: Finished after 187 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "112 Episode: Finished after 178 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "113 Episode: Finished after 187 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "114 Episode: Finished after 175 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "115 Episode: Finished after 189 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "116 Episode: Finished after 166 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "117 Episode: Finished after 147 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "118 Episode: Finished after 184 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "119 Episode: Finished after 159 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "120 Episode: Finished after 171 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "121 Episode: Finished after 156 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "122 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.64\n",
      "123 Episode: Finished after 188 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "124 Episode: Finished after 170 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "125 Episode: Finished after 165 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "126 Episode: Finished after 179 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "127 Episode: Finished after 196 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "128 Episode: Finished after 185 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "129 Episode: Finished after 183 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "130 Episode: Finished after 195 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "131 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.62\n",
      "132 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.6\n",
      "133 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.58\n",
      "134 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.56\n",
      "135 Episode: Finished after 200 time steps  Episode Reward:[1.]/Mean:-0.54\n",
      "136 Episode: Finished after 163 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "137 Episode: Finished after 25 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "138 Episode: Finished after 18 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "139 Episode: Finished after 12 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "140 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "141 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "142 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "143 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "144 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "145 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "146 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "147 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "148 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.54\n",
      "149 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.56\n",
      "150 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.56\n",
      "151 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.56\n",
      "152 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.58\n",
      "153 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.6\n",
      "154 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-0.62\n",
      "155 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.62\n",
      "156 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "157 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "158 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.64\n",
      "159 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.66\n",
      "160 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "161 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "162 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "163 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "164 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "165 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "166 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "167 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "168 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "169 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "170 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "171 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "172 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "173 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "174 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "175 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "176 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "177 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "178 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "179 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "180 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "181 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "182 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "183 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "184 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "185 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.68\n",
      "186 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.7\n",
      "187 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.72\n",
      "188 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.72\n",
      "189 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.72\n",
      "190 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.72\n",
      "191 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.72\n",
      "192 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-0.72\n",
      "193 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.74\n",
      "194 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.74\n",
      "195 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.74\n",
      "196 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.74\n",
      "197 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.74\n",
      "198 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.74\n",
      "199 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.76\n",
      "200 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.78\n",
      "201 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.78\n",
      "202 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.8\n",
      "203 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.8\n",
      "204 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.8\n",
      "205 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.8\n",
      "206 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.82\n",
      "207 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.84\n",
      "208 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.86\n",
      "209 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "210 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "211 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "212 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "213 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "214 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "215 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "216 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "217 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "218 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "219 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "220 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.88\n",
      "222 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.9\n",
      "223 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.9\n",
      "224 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.9\n",
      "225 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.9\n",
      "226 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.9\n",
      "227 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.9\n",
      "228 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-0.9\n",
      "229 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.9\n",
      "230 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.9\n",
      "231 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.92\n",
      "232 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-0.94\n",
      "233 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.96\n",
      "234 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-0.98\n",
      "235 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "236 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "237 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "238 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "239 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "240 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "241 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "242 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "243 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "244 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "245 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "246 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "247 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "248 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "249 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "250 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "251 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "252 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "253 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "254 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "255 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "256 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "257 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "258 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "259 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "260 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "261 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "262 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "263 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "264 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "265 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "266 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "267 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "268 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "269 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "270 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "271 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "272 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "273 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "274 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "275 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "276 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "277 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "278 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "279 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "280 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "281 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "282 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "283 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "284 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "285 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "286 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "287 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "288 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "289 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "290 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "291 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "292 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "293 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "294 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "295 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "296 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "297 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "298 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "299 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "300 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "301 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "302 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "303 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "304 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "305 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "306 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "307 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "308 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "309 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "310 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "311 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "312 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "313 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "314 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "315 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "316 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "317 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "318 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "319 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "320 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "321 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "322 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "323 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "324 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "325 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "326 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "327 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "328 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "329 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "330 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "331 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "332 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "334 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "335 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "336 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "337 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "338 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "339 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "340 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "341 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "342 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "343 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "344 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "345 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "346 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "347 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "348 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "349 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "350 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "351 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "352 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "353 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "354 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "355 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "356 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "357 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "358 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "359 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "360 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "361 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "362 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "363 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "364 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "365 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "366 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "367 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "368 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "369 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "370 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "371 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "372 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "373 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "374 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "375 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "376 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "377 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "378 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "379 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "380 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "381 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "382 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "383 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "384 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "385 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "386 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "387 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "388 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "389 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "390 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "391 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "392 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "393 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "394 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "395 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "396 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "397 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "398 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "399 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "400 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "401 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "402 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "403 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "404 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "405 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "406 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "407 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "408 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "409 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "410 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "411 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "412 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "413 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "414 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "415 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "416 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "417 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "418 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "419 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "420 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "421 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "422 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "423 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "424 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "425 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "426 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "427 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "428 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "429 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "430 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "431 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "432 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "433 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "434 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "435 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "436 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "437 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "438 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "439 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "440 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "441 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "442 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "443 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "445 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "446 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "447 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "448 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "449 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "450 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "451 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "452 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "453 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "454 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "455 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "456 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "457 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "458 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "459 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "460 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "461 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "462 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "463 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "464 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "465 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "466 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "467 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "468 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "469 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "470 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "471 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "472 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "473 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "474 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "475 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "476 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "477 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "478 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "479 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "480 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "481 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "482 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "483 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "484 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "485 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "486 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "487 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "488 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "489 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "490 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "491 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "492 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "493 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "494 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "495 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "496 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "497 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "498 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "499 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "500 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "501 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "502 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "503 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "504 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "505 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "506 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "507 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "508 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "509 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "510 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "511 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "512 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "513 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "514 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "515 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "516 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "517 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "518 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "519 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "520 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "521 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "522 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "523 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "524 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "525 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "526 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "527 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "528 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "529 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "530 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "531 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "532 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "533 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "534 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "535 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "536 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "537 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "538 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "539 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "540 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "541 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "542 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "543 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "544 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "545 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "546 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "547 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "548 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "549 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "550 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "551 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "552 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "553 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "554 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "555 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "556 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "557 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "558 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "559 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "560 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "562 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "563 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "564 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "565 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "566 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "567 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "568 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "569 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "570 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "571 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "572 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "573 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "574 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "575 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "576 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "577 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "578 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "579 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "580 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "581 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "582 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "583 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "584 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "585 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "586 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "587 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "588 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "589 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "590 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "591 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "592 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "593 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "594 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "595 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "596 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "597 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "598 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "599 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "600 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "601 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "602 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "603 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "604 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "605 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "606 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "607 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "608 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "609 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "610 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "611 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "612 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "613 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "614 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "615 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "616 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "617 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "618 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "619 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "620 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "621 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "622 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "623 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "624 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "625 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "626 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "627 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "628 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "629 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "630 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "631 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "632 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "633 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "634 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "635 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "636 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "637 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "638 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "639 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "640 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "641 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "642 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "643 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "644 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "645 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "646 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "647 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "648 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "649 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "650 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "651 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "652 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "653 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "654 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "655 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "656 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "657 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "658 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "659 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "660 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "661 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "662 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "663 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "664 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "665 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "666 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "667 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "668 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "669 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "670 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "671 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "673 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "674 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "675 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "676 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "677 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "678 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "679 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "680 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "681 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "682 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "683 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "684 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "685 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "686 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "687 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "688 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "689 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "690 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "691 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "692 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "693 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "694 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "695 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "696 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "697 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "698 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "699 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "700 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "701 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "702 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "703 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "704 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "705 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "706 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "707 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "708 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "709 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "710 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "711 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "712 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "713 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "714 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "715 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "716 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "717 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "718 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "719 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "720 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "721 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "722 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "723 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "724 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "725 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "726 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "727 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "728 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "729 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "730 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "731 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "732 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "733 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "734 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "735 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "736 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "737 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "738 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "739 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "740 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "741 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "742 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "743 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "744 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "745 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "746 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "747 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "748 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "749 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "750 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "751 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "752 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "753 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "754 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "755 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "756 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "757 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "758 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "759 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "760 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "761 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "762 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "763 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "764 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "765 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "766 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "767 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "768 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "769 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "770 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "771 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "772 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "773 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "774 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "775 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "776 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "777 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "778 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "779 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "780 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "781 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "782 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "783 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "785 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "786 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "787 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "788 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "789 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "790 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "791 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "792 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "793 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "794 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "795 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "796 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "797 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "798 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "799 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "800 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "801 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "802 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "803 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "804 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "805 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "806 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "807 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "808 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "809 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "810 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "811 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "812 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "813 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "814 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "815 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "816 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "817 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "818 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "819 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "820 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "821 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "822 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "823 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "824 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "825 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "826 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "827 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "828 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "829 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "830 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "831 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "832 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "833 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "834 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "835 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "836 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "837 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "838 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "839 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "840 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "841 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "842 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "843 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "844 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "845 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "846 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "847 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "848 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "849 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "850 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "851 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "852 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "853 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "854 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "855 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "856 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "857 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "858 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "859 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "860 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "861 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "862 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "863 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "864 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "865 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "866 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "867 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "868 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "869 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "870 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "871 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "872 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "873 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "874 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "875 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "876 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "877 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "878 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "879 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "880 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "881 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "882 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "883 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "884 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "885 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "886 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "887 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "888 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "889 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "890 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "891 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "892 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "893 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "894 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "895 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "896 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "897 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "899 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "900 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "901 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "902 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "903 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "904 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "905 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "906 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "907 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "908 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "909 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "910 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "911 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "912 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "913 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "914 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "915 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "916 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "917 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "918 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "919 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "920 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "921 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "922 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "923 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "924 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "925 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "926 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "927 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "928 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "929 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "930 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "931 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "932 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "933 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "934 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "935 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "936 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "937 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "938 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "939 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "940 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "941 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "942 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "943 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "944 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "945 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "946 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "947 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "948 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "949 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "950 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "951 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "952 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "953 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "954 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "955 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "956 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "957 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "958 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "959 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "960 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "961 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "962 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "963 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "964 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "965 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "966 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "967 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "968 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "969 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "970 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "971 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "972 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "973 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "974 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "975 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "976 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "977 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "978 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "979 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "980 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "981 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "982 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "983 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "984 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "985 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "986 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "987 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "988 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "989 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "990 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "991 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "992 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "993 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "994 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "995 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "996 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "997 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "998 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "999 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1000 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1001 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1002 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1003 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1004 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1005 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1006 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1007 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1008 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1010 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1011 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1012 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1013 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1014 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1015 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1016 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1017 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1018 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1019 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1020 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1021 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1022 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1023 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1024 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1025 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1026 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1027 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1028 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1029 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1030 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1031 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1032 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1033 Episode: Finished after 11 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1034 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1035 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1036 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1037 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1038 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1039 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1040 Episode: Finished after 12 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1041 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1042 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1043 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1044 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1045 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1046 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1047 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1048 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1049 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1050 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1051 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1052 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1053 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1054 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1055 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1056 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1057 Episode: Finished after 8 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1058 Episode: Finished after 9 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1059 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1060 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n",
      "1061 Episode: Finished after 10 time steps  Episode Reward:[-1.]/Mean:-1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-4bf7f6ad7ab7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# main クラス\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcartpole_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcartpole_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-1fae2a4f0096>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;31m# Experience ReplayでQ関数を更新する\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_q_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;31m# 観測の更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-c8d687ed76db>\u001b[0m in \u001b[0;36mupdate_q_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_q_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;34m'''Q関数を更新する'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-c598bd276ab9>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# catはConcatenates（結合）のことです。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mstate_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0maction_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mreward_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main クラス\n",
    "cartpole_env = Environment()\n",
    "cartpole_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if step < 199:\n",
    "                    if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                        state_next = None  # 次の状態はないので、Noneを格納\n",
    "                        reward = torch.FloatTensor([-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "                        episode_reward+=reward.numpy()\n",
    "                    else:\n",
    "                        reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                        episode_reward+=reward.numpy()\n",
    "                        state_next = observation_next  # 観測をそのまま状態とする\n",
    "                        state_next = torch.from_numpy(state_next).type(torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                        state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
    "                else:\n",
    "                    if done:\n",
    "                        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "                        episode_reward+=reward.numpy()\n",
    "                        state_next = None\n",
    "                    else:\n",
    "                        state_next = observation_next  # 観測をそのまま状態とする\n",
    "                        state_next = torch.from_numpy(state_next).type(torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                        state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
